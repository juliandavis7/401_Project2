{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary of priors (one key, value pair for each category)\n",
    "def compute_priors(df, response):\n",
    "    y = df[response]\n",
    "    priors_dict = {}\n",
    "    priors = dict(y.value_counts(normalize=True))\n",
    "    return priors\n",
    "\n",
    "# returns a dictionary of mu vectors (one key, value pair for each category)\n",
    "def compute_mu_vectors(df, response):\n",
    "    y = df[response]\n",
    "    mu_vectors = {}\n",
    "    for category_k in y.unique():\n",
    "        df_k = df[y == category_k] # df with y = category_k\n",
    "        X_k = df_k.drop(response, axis=1)\n",
    "        mu_vectors[category_k] = dict(X_k.mean())\n",
    "    return mu_vectors\n",
    "\n",
    "# returns the inverse of the covariance matrix\n",
    "def compute_inv_sigma(df, response):\n",
    "    X = df.drop(response, axis=1)\n",
    "    return inv(X.cov())\n",
    "\n",
    "# returns the classification of a single obs\n",
    "def classify_obs(x_i, y, mu_vectors, priors, inv_sigma):\n",
    "        prob_dict = {}\n",
    "        for category_k in y.unique():\n",
    "            mu_k = pd.Series(mu_vectors[category_k]).to_numpy()\n",
    "            first_term = x_i.transpose().dot(inv_sigma).dot(mu_k)\n",
    "            second_term = .5 * mu_k.transpose().dot(inv_sigma).dot(mu_k)\n",
    "            third_term = np.log(priors[category_k])\n",
    "            prob_k = first_term - second_term + third_term\n",
    "            prob_dict[category_k] = prob_k\n",
    "\n",
    "        best_class, max_prob = next(iter(prob_dict.items()))\n",
    "        for class_k, prob_k in prob_dict.items():\n",
    "            if max_prob < prob_k:\n",
    "                max_prob = prob_k\n",
    "                best_class = class_k\n",
    "        return best_class\n",
    "\n",
    "class LDA:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        df_train = X_train.copy()\n",
    "        response = y_train.name\n",
    "        df_train[response] = y_train\n",
    "        self.y = df_train[response]\n",
    "        self.priors = compute_priors(df_train, response)\n",
    "        self.mu_vectors = compute_mu_vectors(df_train, response)\n",
    "        self.inv_sigma = compute_inv_sigma(df_train, response)\n",
    "        \n",
    "    def predict(self, df_test):\n",
    "        y_pred = {}\n",
    "        for i in range(len(df_test)):\n",
    "            x_i = df_test.loc[i, :].to_numpy()\n",
    "            y_pred[i] = classify_obs(x_i, self.y, self.mu_vectors, self.priors, self.inv_sigma)\n",
    "        return pd.Series(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Compute Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_confusion_matrix(y_pred, y_test):\n",
    "    res = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test})\n",
    "\n",
    "    res_positives = res[res[\"Actual\"] == 1]\n",
    "    res_negatives = res[res[\"Actual\"] == -1]\n",
    "\n",
    "    positives_dict = dict(res_positives[\"Prediction\"].value_counts(normalize=True))\n",
    "    TPs = positives_dict[1]\n",
    "    FNs = positives_dict[-1]\n",
    "\n",
    "    negatives_dict = dict(res_negatives[\"Prediction\"].value_counts(normalize=True))\n",
    "    TNs = negatives_dict[-1]\n",
    "    FPs = negatives_dict[1]\n",
    "\n",
    "    positives = pd.Series([TPs, FNs])\n",
    "    negatives = pd.Series([FPs, TNs])\n",
    "\n",
    "    confusion_matrix = pd.DataFrame({\"Actually Positive\": positives, \"Actually Negative\": negatives})\n",
    "    confusion_matrix.rename({0: \"Prediced Positive\", 1: \"Predicted Negative\"}, inplace=True)\n",
    "    return confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test (Feature Set #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train1.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "df_test = pd.read_csv(\"test1.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "X_train = df_train.drop(\"Label\", axis=1)\n",
    "y_train = df_train[\"Label\"]\n",
    "X_test = df_test.drop(\"Label\", axis=1)\n",
    "y_test = df_test[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Classifier Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDA()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = produce_confusion_matrix(y_pred, y_test)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()\n",
    "X_test2 = X_test.copy()\n",
    "y_train2 = y_train.copy()\n",
    "y_test2 = y_test.copy()\n",
    "\n",
    "X_train2[\"Interaction_posc_negc\"] = X_train2.Positive_counts*X_train2.Negative_counts\n",
    "X_test2[\"Interaction_posc_negc\"] = X_test2.Positive_counts*X_test2.Negative_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73484"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LDA()\n",
    "model2.fit(X_train2, y_train2)\n",
    "y_pred2 = model2.predict(X_test2)\n",
    "acc = (y_pred2 == y_test2).mean()\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actually Positive</th>\n",
       "      <th>Actually Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prediced Positive</th>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.25944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>0.2792</td>\n",
       "      <td>0.74056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actually Positive  Actually Negative\n",
       "Prediced Positive              0.7208            0.25944\n",
       "Predicted Negative             0.2792            0.74056"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = produce_confusion_matrix(y_pred, y_test)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squaring/Cubing Terms did not further improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test (Feature Set #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (minutes) elapsed for this cell: 0.44982966064999347\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "df_train = pd.read_csv(\"train2.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "df_test = pd.read_csv(\"test2.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove columns that are too sparse, allows LDA to perform matrix algebra\n",
    "cols_to_drop = []\n",
    "df = df_train.drop(\"Label\", axis=1)\n",
    "for col in df:\n",
    "    num_vals = len(df) - df[col].value_counts()[0]\n",
    "    if num_vals < 3:\n",
    "        cols_to_drop.append(col)\n",
    "len(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "df_test.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "X_train = df_train.drop(\"Label\", axis=1)\n",
    "y_train = df_train[\"Label\"]\n",
    "X_test = df_test.drop(\"Label\", axis=1)\n",
    "y_test = df_test[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Classifier Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88752\n",
      "Time (minutes) elapsed for this cell: 8.506495890266766\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "model = LDA()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actually Positive</th>\n",
       "      <th>Actually Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prediced Positive</th>\n",
       "      <td>0.89736</td>\n",
       "      <td>0.12232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>0.10264</td>\n",
       "      <td>0.87768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actually Positive  Actually Negative\n",
       "Prediced Positive             0.89736            0.12232\n",
       "Predicted Negative            0.10264            0.87768"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = produce_confusion_matrix(y_pred, y_test)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test (Stanford Feature Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "df_train = pd.read_csv(\"stanford_train.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "df_test = pd.read_csv(\"stanford_test.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "X_train = df_train.drop(\"Label\", axis=1)\n",
    "y_train = df_train[\"Label\"]\n",
    "X_test = df_test.drop(\"Label\", axis=1)\n",
    "y_test = df_test[\"Label\"]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Classifier Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49692"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "model = LDA()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"Time (minutes) elapsed for this cell:\", elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actually Positive</th>\n",
       "      <th>Actually Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prediced Positive</th>\n",
       "      <td>0.39904</td>\n",
       "      <td>0.4052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>0.60096</td>\n",
       "      <td>0.5948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actually Positive  Actually Negative\n",
       "Prediced Positive             0.39904             0.4052\n",
       "Predicted Negative            0.60096             0.5948"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = produce_confusion_matrix(y_pred, y_test)\n",
    "confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
