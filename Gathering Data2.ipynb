{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter-jdavi104/proj2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir = os.getcwd()\n",
    "maindir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt',\n",
       " 'neg',\n",
       " 'labeledBow.feat',\n",
       " 'unsupBow.feat',\n",
       " 'unsup']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/../../../datasets/aclImdb/train\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"pos\")\n",
    "pos_files = os.listdir()\n",
    "pos_reviews = []\n",
    "for i in pos_files:\n",
    "    f = open(i,'r')\n",
    "    rev = f.read()\n",
    "    pos_reviews.append(rev)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../neg\")\n",
    "neg_files = os.listdir()\n",
    "neg_reviews = []\n",
    "for i in neg_files:\n",
    "    f = open(i,'r')\n",
    "    rev = f.read()\n",
    "    neg_reviews.append(rev)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2893_10.txt</td>\n",
       "      <td>Walt Disney's CINDERELLA takes a story everybo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7944_9.txt</td>\n",
       "      <td>Have you ever, or do you have, a pet who's bee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11725_10.txt</td>\n",
       "      <td>I suck at gratuitous Boob references, so i'm j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1587_10.txt</td>\n",
       "      <td>Does anyone know, where I can see or download ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10297_8.txt</td>\n",
       "      <td>Well not actually. This movie is very entertai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           File                                             Review  Label\n",
       "0   2893_10.txt  Walt Disney's CINDERELLA takes a story everybo...      1\n",
       "1    7944_9.txt  Have you ever, or do you have, a pet who's bee...      1\n",
       "2  11725_10.txt  I suck at gratuitous Boob references, so i'm j...      1\n",
       "3   1587_10.txt  Does anyone know, where I can see or download ...      1\n",
       "4   10297_8.txt  Well not actually. This movie is very entertai...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame({\"File\":pos_files,\"Review\":pos_reviews,\"Label\":[1]*len(pos_files)})\n",
    "neg_df = pd.DataFrame({\"File\":neg_files,\"Review\":neg_reviews,\"Label\":[-1]*len(neg_files)})\n",
    "\n",
    "Train_df = pd.concat([pos_df,neg_df])\n",
    "Train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter-\n",
      "[nltk_data]     jdavi104/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter-jdavi104/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for rev in pos_df[\"Review\"]:\n",
    "    \n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', rev)\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())   \n",
    "            \n",
    "for rev in neg_df[\"Review\"]:\n",
    "    \n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', rev)\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives. \n",
    "BOW = nltk.FreqDist(all_words)\n",
    "\n",
    "# listing the 5000 most frequent words\n",
    "word_features = list(BOW.keys())[:5000]\n",
    "word_features[0], word_features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = []\n",
    "\n",
    "for i, row in Train_df.iterrows():\n",
    "    featuresets.append((find_features(row[\"Review\"]), row[\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(featuresets)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.json_normalize(df[0])\n",
    "df_train = df_train.astype(int)\n",
    "df_train[\"Label\"] = df[1]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(maindir)\n",
    "df_train.to_csv(\"train2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/../../../datasets/aclImdb/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"pos\")\n",
    "pos_files = os.listdir()\n",
    "pos_reviews = []\n",
    "for i in pos_files:\n",
    "    f = open(i,'r')\n",
    "    rev = f.read()\n",
    "    pos_reviews.append(rev)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../neg\")\n",
    "neg_files = os.listdir()\n",
    "neg_reviews = []\n",
    "for i in neg_files:\n",
    "    f = open(i,'r')\n",
    "    rev = f.read()\n",
    "    neg_reviews.append(rev)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame({\"File\":pos_files,\"Review\":pos_reviews,\"Label\":[1]*len(pos_files)})\n",
    "neg_df = pd.DataFrame({\"File\":neg_files,\"Review\":neg_reviews,\"Label\":[-1]*len(neg_files)})\n",
    "\n",
    "Test_df = pd.concat([pos_df,neg_df])\n",
    "Test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in Test_df.iterrows():\n",
    "    featuresets.append((find_features(row[\"Review\"]), row[\"Label\"]))\n",
    "    \n",
    "df2 = pd.DataFrame.from_dict(featuresets)\n",
    "df2.head()\n",
    "\n",
    "df_test = pd.json_normalize(df[0])\n",
    "df_test = df_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Label\"] = df2[1]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(maindir)\n",
    "df_test.to_csv(\"test2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
